{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e8af9523-9ae3-4d8d-8e75-8caf2b8364d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            0\n",
      "I enjoy hiking and camping in the mountains                        0\n",
      "I like to read books and watch movies                              1\n",
      "I prefer playing video games over sports                           0\n",
      "I love listening to music and going to concerts                    1\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " playing\n",
      " the\n",
      " weekends\n",
      " on\n",
      " football\n",
      " video\n",
      " sports\n",
      " prefer\n",
      " over\n",
      " games\n",
      "\n",
      "Cluster 1:\n",
      " to\n",
      " and\n",
      " read\n",
      " watch\n",
      " movies\n",
      " like\n",
      " books\n",
      " concerts\n",
      " going\n",
      " music\n",
      "\n",
      "Purity: 0.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from tabulate import tabulate \n",
    "from collections import Counter\n",
    "\n",
    "# Sample dataset\n",
    "dataset = [\n",
    "    \"I love playing football on the weekends\",\n",
    "    \"I enjoy hiking and camping in the mountains\",\n",
    "    \"I like to read books and watch movies\",\n",
    "    \"I prefer playing video games over sports\",\n",
    "    \"I love listening to music and going to concerts\"\n",
    "]\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dataset)\n",
    "\n",
    "# Clustering\n",
    "k = 2\n",
    "km = KMeans(n_clusters=k, random_state=42)\n",
    "km.fit(X)\n",
    "\n",
    "# Predict clusters\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Display the document and its predicted cluster in a table\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Optional: calculate purity (if you had ground-truth labels)\n",
    "# Since we don't have true labels, this section is not applicable here\n",
    "\n",
    "# Print top terms per cluster\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(k):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(f\" {terms[ind]}\")\n",
    "    print()\n",
    "\n",
    "# Calculate purity \n",
    "total_samples = len(y_pred) \n",
    "cluster_label_counts = [Counter(y_pred)] \n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples \n",
    "print(\"Purity:\", purity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d8a56360-2ec4-4672-b923-5bb0b9e44726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Afiq\n",
      "[nltk_data]     Fikri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Afiq\n",
      "[nltk_data]     Fikri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Afiq\n",
      "[nltk_data]     Fikri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document                                Preprocessed                          Predicted Cluster\n",
      "-----------------------------------------------  ----------------------------------  -------------------\n",
      "I love playing football on the weekends          love playing football weekend                         1\n",
      "I enjoy hiking and camping in the mountains      enjoy hiking camping mountain                         0\n",
      "I like to read books and watch movies            like read book watch movie                            1\n",
      "I prefer playing video games over sports         prefer playing video game sport                       1\n",
      "I love listening to music and going to concerts  love listening music going concert                    1\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " camping\n",
      " enjoy\n",
      " hiking\n",
      " mountain\n",
      " weekend\n",
      " listening\n",
      " concert\n",
      " football\n",
      " game\n",
      " going\n",
      "\n",
      "Cluster 1:\n",
      " love\n",
      " playing\n",
      " football\n",
      " weekend\n",
      " going\n",
      " sport\n",
      " music\n",
      " concert\n",
      " video\n",
      " game\n",
      "\n",
      "Purity (estimated): 0.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample dataset\n",
    "dataset = [\n",
    "    \"I love playing football on the weekends\",\n",
    "    \"I enjoy hiking and camping in the mountains\",\n",
    "    \"I like to read books and watch movies\",\n",
    "    \"I prefer playing video games over sports\",\n",
    "    \"I love listening to music and going to concerts\"\n",
    "]\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "preprocessed_dataset = [preprocess(doc) for doc in dataset]\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_dataset)\n",
    "\n",
    "# Clustering\n",
    "k = 2\n",
    "km = KMeans(n_clusters=k, random_state=42)\n",
    "km.fit(X)\n",
    "\n",
    "# Predict clusters\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Display the document and its predicted cluster in a table\n",
    "table_data = [[\"Original Document\", \"Preprocessed\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[orig, prep, cluster] for orig, prep, cluster in zip(dataset, preprocessed_dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Print top terms per cluster\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(k):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(f\" {terms[ind]}\")\n",
    "    print()\n",
    "\n",
    "# Calculate purity (placeholder since no ground-truth labels)\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity (estimated):\", purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3419cc2-089a-4596-a671-4b90d730ae6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
